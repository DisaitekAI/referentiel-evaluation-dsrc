
#  Atelier du 02 février 2020

_Présents : @ClementMayer, @bowni, @SaboniAmine, @celinejacques, Véronique Brun, Tomothée Faucond, Cédric Meurée, Paul Dalous, Jérémie Abiteboul 
Merci à tous !

#Welcome

\url{https://frama.link/substra}
Référentiel : \url{https://github.com/SubstraFoundation/referentiel-ds-responsable-confiance}
Méthode : notes puis intégration au repos.

##Atelier participatif #2 Data Science responsable et de confiance
###Jeudi 6 février 2019 - Agenda

    MATINEE

*9H30 - 10h30 : Accueil co-working éventuel*
*Possibilité pour ceux que cela arrangerait de s’installer à la Paillasse pour travailler (Salle de réunion ouverte - Maison du Libre et des Communs, Wifi, Café)*

10h30 - 11h00 : Accueil des participants

11h00 - 11h30 : Introduction et rappel

   * Rappel des éléments-clés de l’initiative
   * Présentation des participants
11h30 - 12h30 : Revue de toute l’actualité de la Data Science Responsable et de Confiance

   * Actualités : revue et commentaires des actualités / débats sur les thèmes Data Science Responsable et de Confiance, échange sur les nouvelles sources identifiées par les participants pour inspirer l’élaboration du référentiel
   * Evénements : partage autour des événements IA / data science à venir
*12h30 - 14h00 : Repas libre*


    APRES-MIDI

14h00 - 15h30 : Revue des risques et des mesures, partie 1
Revue des risques identifiées lors de la dernière session, identification d’exemples concrets supplémentaires en face de chaque risque, synthèse lecture nouvelles sources d’inspiration, identification de nouveaux risques.

*15h30 - 16h00 : Pause*

16h00 - 17h30 : Revue des risques et des mesures, partie 2
Revue des premières mesures identifiées lors de l’atelier #1 afin de solidifier les formulations, synthèse lecture nouvelles sources d’inspiration, identification de mesures supplémentaires.

*17h30 - 18h30 : Possibilité de travailler sur place pour celles et ceux qui le souhaitent*

Soirée

18h30 - 20h00 : Prolongation des échanges de manière informelle autour d'un verre à La Paillasse.


--

###Matin : Intro

**Tour de table : **

- Eric - Substra Foundation - Réussir à faire un référentiel qui puisse servir à beaucoup d'organisations
- Paul - Consultant chez Octo Technology
- Nathanaël - Substra foundation
- Céline - Apricity
- Anne-Sophie - AMASAI
- Jérémie - DreamQuark
- Véronique - Ingénieure de formation en transition professionnelle, ex Phillips / eCommerce
- Cédric - Aivision
- Timothée - Aivision
- Amine - Octo Technology
- Clément - Substra Foundation

Lieu :
    - Maison des libres et du commun ! Lieu dédié à l'Open Source <3

**Actualité / événements : **

**- Actu 1 : Ethique vs. Responsable et de Confiance **
Article de Tom Chatfield : \url{https://onezero.medium.com/theres-no-such-thing-as-ethical-a-i-38891899261d}
Linkedin - Jerôme Fortias : \url{https://www.linkedin.com/pulse/ia-et-%C3%A9thique-calmons-nous-jerome-fortias/}
blog post d'Eric : \url{https://www.linkedin.com/pulse/ia-et-%C3%A9thique-ou-plut%C3%B4t-responsabilit%C3%A9-confiance-eric-boniface/}

Praticiens chevronnés semblent énervés...
Utilisation du terme éthique problématique - pas d'IA et Ethique mais problèmes sur les usages.
Garder une boite noire mais ne juger que les usages ?
Qu'est ce qu'être éthique, peut on en choisir une ?

   * - Usage ? bien / pas bien
   * - Grands principes
   * - Objectifs morauxLecture sur l'éthique (référence à Max Weber) : \url{https://www.erudit.org/fr/revues/ltp/1996-v52-n2-ltp2155/401006ar.pdf}

OSCE : sécurité européenne - conclusion : "intelligence artificielle n'existe pas et donc on ne s'y intéresse pas"
Problématique de la reconnaissance artificielle
--> difficulté de comprendre ce qu'est l'IA et l'usage
OSCE : \url{https://www.osce.org/}

Technologie singulière / qu'est ce qui est singulier :
    - Modèle "appris", sans être capable de "comprendre" vs. avant règles explicitement définis
    - problème résultant :

   *         - Gouvernance...
   *
Prévision météo ? quelle est la responsabilité ?

Apple card : plafond autorisé entre homme et femme dans les couples --> problème : qui est responsable ? Pendant plusieurs jours pas de réponse - "pas nous c'est lalgo" - idem côté Goldman Sachs, créateur de l'algo.
COMPAS : système sur liberté conditionnelle / récidive - problème sur la metric de Fairness. \url{https://en.wikipedia.org/wiki/COMPAS\_(software)}

Cas de la police de Chicago débranché - lien avec la performance

Analogie intéressante :* plan de vol - condition d'utilisation d'un appareil *
\url{http://www.mutationstechnologiques.fr/la-mecanique-algorithmique-de-la-police-predictive-a-chicago/}

Exemple de problème : homme / femme - pondération négative

Modèle d'IA : aujourd'hui très simple - Tensor Flow et super performance --> mais quid de l'identification de biais ? comment ? Pas de recul sur ce qui est fait (pas de distance critique).

Analogie : sécurité informatique : outil téléchargé rapidement car pratique. Jusqu'au jour où problème sur les licenses achetées, sécurité, etc ... --> processus / référentiel

Data Science : technique ok, maintenant il est temps de mettre en place un environnement autour pour mettre de la confiance.

Problème d'esprit critique autour de l'informatique / IA

Mais bienfait de l'IA : persistence (pas de sentiments)

Problème des systèmes fermés :
 Exemple si problème, pas de possibilité de changer dans le système, prise en compte de l'*input* humain face à une décision automatisée
**Mesure potentielle : pouvoir aller contre l'IA ?**

Responsable : notion juridique ?

**- Actu 2 :  Google**

Intérêt pour de la régulation

Faut-il être un peu méfiant ? lobby

Gap entre RGPD 2012 et 2018 --> ce qui a été décidé n'est plus valable.

**- Actu 3 :  ISO sur l'IA **

\url{https://blog.iec.ch/2019/11/international-standards-committee-on-ai-ecosystem-achieves-milestone-and-launches-new-areas-of-study/}

Très complexe en terme de méthode mais également concept.
Timing ? Très long
--> possibilité de cohabiter

ISO : plutôt suivi et documentation ? approche pas forcément responsable

**- Actu 4 :  Health Data Hub **

\url{https://www.cnb.avocat.fr/sites/default/files/11.cnb-mo2020-01-11\_ldh\_health\_data\_hubfinal-p.pdf}
Problème lié à Azure / Microsoft et le Cloud Act.

**- Actu 5 : Proposition de loi sur la charte IA des algorithme **

Lien : \url{http://www.assemblee-nationale.fr/15/pdf/propositions/pion2585.pdf}

**- Actu 6 : Etalab **

Rapport sur la responsabilité des algorithmes publics
\url{https://www.etalab.gouv.fr/algorithmes-publics-des-eleves-de-lena-formulent-une-serie-de-recommandations-sur-les-enjeux-dethique-et-de-responsabilite}

\_\_\_
Lunch Break nom nom nom
\_\_\_

###Après-midi : On rentre dans le dur !

    Revue des risques !
\url{https://github.com/SubstraFoundation/referentiel-ds-responsable-confiance/blob/master/referentiel.md}

**Partie 1 : **

**Définition à donner - Données confidentielles : données qui a de la valeur pas nécessairement pro (exemple : pharma.) **
Ré-enditification de Nature.  \url{https://www.nature.com/articles/s41467-019-10933-3/} : "Using our model, we find that 99.98% of Americans would be correctly re-identified in any dataset using 15 demographic attributes."
**Offusquer > Anonymisation **

Elargir pas que changement de réglementation mais par exemple double exposition
Contradiction entre les différentes réglementations : FATCA (Foreign Account Tax Compliance Act) contredit RGPD : \url{https://www.cnil.fr/fr/cnil-direct/question/loi-fatca-que-faire}

Exposer : qu'est-ce que cela signifie ?
Précision vs. viser large ? réponse pas évidente ...
Précision = difficulté d'évaluer au niveau d'une organisation.
--> voir au niveau des mesures.


**Trade off: exposition vs. généologie **
cf. \url{https://ai.facebook.com/blog/using-radioactive-data-to-detect-if-a-data-set-was-used-for-training/}

Utilisateur doit être informé de l'utilisation de ses données - rendre des comptes.

**Partie 2 : **

**Manière dont le modèle va être construit peut entrainer des biais. **
doc2vec

Autre exemple : features / individus :
exemple : intégration de l'ethnie
Exemple à trouver

Social cooling :
Auto-Censure des utilisateurs de RS

Empoisonnement de données --> cas à identifier

modèle drift : exemple à trouver dans le trading
Maintenance prédictive : problème sur des capteurs
**--> drift modèle vs. drift données **

autre exemple : **Face ID / reconnaissance faciale **--> marche pas en Chine
\url{https://www.thesun.co.uk/news/5182512/chinese-users-claim-iphonex-face-recognition-cant-tell-them-apart/}
autre exemple : \url{https://arxiv.org/pdf/1902.11097.pdf} biais dans les systèmes de reconnaissance

**Partie 3 : **
**Apprentissage supervisé vs renforcé ? **
apprentissage par renforcement --> change beaucoup la notion de responsabilité car du coup l'algo / modèle peut être mis dans des conditions non anticipées => continuum de responsabilités pour les algos auto-apprenants

Linkedin : utilisation d'un algo qui peut discriminer sans le savoir.
+ auto-optimisation - manipuler des algos.
**Manipulation / exploitation d'un algo**

**Valeur de responsabilité au sein d'une organisation ? --> partie prenante interne à une organisation**

 (Singularité : règles non écrites par des humains)

** Partie 4 :**

Entrainement à l'origine vs. entrainement à chaque fois
IA justifié vs. non justifié ? systèmes non autorisés ?
(Deepmind: refroidissement)
(Off topic: \url{https://www.electricitymap.org)}

Job :
Exemple : La Redoute sur les fiches produits  
Exemple : traders au chômage
Exemple : En attendant les robots d'A. Cassili \url{http://www.seuil.com/ouvrage/en-attendant-les-robots-antonio-a-casilli/9782021401882}

Ferme à clics : impacts sociaux / digital sociaux

Travail des personnes / captcha --> travail déguisé  

**Effets négatifs : **
**OpenAI : GPT-2 **\url{**https://openai.com/blog/gpt-2-1-5b-release/**}
**Deepfake  **

Rajouter Cambridge Analytica comme exemple. --> **malfaisant pas que d'un modèle mais de tout une technologie (allant jusqu'à un "acte de guerre")**

**Partie 5 :**

**gouvernance globale : intelligent organisation **
\url{https://www.amazon.fr/Intelligent-Organisation-Realising-value-information/dp/1138847070}
+ Evgeny Morosov: \url{https://slate.com/author/evgeny-morozov}


**Divers :**

**Algorithme développé par des gens - deviennent référence --> biais --> difficile car utilisateurs <> concepteur**
**exemple : Resnet dans l'automobile**
**Risque avec NLP**


Système Boeing : auditeur vs. créateur

New :

**Risque réglementaire : Pas s'interroger sur la réglementation (exemple reproductible + explications) **

EDF : problème sur la démonstration - risque de pas pouvoir tester en nature.

**Transfert de performance de dataset à un autre : exemple publier un papier sur un dataset donné / robustness d'un algo --> responsabilité ?  **

Continuous delivery for machine learning
\url{https://martinfowler.com/articles/cd4ml.html}


    Revue des mesures !

**T1 :**

OWASP du Machine Learning? soutTop Five risks: sout\url{southttps://wiki.owasp.org/index.php/OWASP\_Top\_5\_Machine\_Learning\_Riskssout} (KO). See: \url{https://github.com/OWASP/Top-5-Machine-Learning-Risks/blob/master/Top%205%20Machine%20Learning%20Risks.md}

**Mesure : est-ce que j'ai besoin de collecter des données personnelles ? minimisation des données, y compris quitte à détériorer la performance. **

**collaboration avec les autorités : exemple inference Attaque - ANSSII**

[A lire : THE ML ENGINEER : \url{https://ethical.institute/index.html}]

[Bon exemples ? comment identifier - comment les décrire]

**T1 - iV et V à rassembler. **

**T2 : **

[Biais de genre  : Invisible Women: Exposing Data Bias in a World Designed for Men
\url{https://www.amazon.fr/Invisible-Women-Exposing-World-Designed/dp/1784742929/}

Fairness: au niveau individuel ? --> "pour quelqu'un de très proche le résultat doit être très proche".

[exemple : utilisation de la donnée Kasher avec des populations différentes]

Biais : pas forcément négatif - des biais peuvent être exploités de manière positive

iii : rechercher des biais

[construction des algorithmes en fonction des data scientists]
**Données synthétiques : manque le risque **

**T3 :**

**i : point d'attention - collaboration - risque d'overfit quand test pas divisé de la même manière. **
**(risque de contamination) **
Deux point :
    - Quand on a fait "l'erreur" : comment identifier ? Généalogie dans les travaux
    - Quand on a la donnée à deux endroits

**Pluralité de la base des tests ? **

iii : préciser

**modèle en production : même sur modèle fix, possibilité d'avoir des données qui ne sont plus dans le domaine. exemple : variable plus renseignée **
CI/CD pipeline sur les données (+ versioning)
**Suivre si entre mise en place du modèle et actuel on est toujours bon **

Mesure le PSI : distribution des scores reste stable dans le temps.

**Ajout de contrôle aléatoire humain - conforme à ce qui est attendu.**
**Mais quid quand c'est complexe ? --> utilisation d'autres mesures. combinaison.  **

mise en place de l'indécision (voir plus loin)

**+ aborder les questions à chaque nouvelle application. **
Pas que au moment du design.

[industrialiser la machine learning] (CD/CI)

Maintenir apprentissage continu spécifique

Problème : est-ce que le test doit évoluer ou non ?


**T4 : **

Model / Data lineage \url{https://fr.wikipedia.org/wiki/Data\_Lineage}

Chaque acteur doit avoir un outil d'interpretabilité spécifique

**Ajouter Limites d'utilisation en corollaire.**

**T5 : **

**Registre des modèles prédictifs (cf RGPD) **
(risque de perte de maitrise)
Evaluation des risques (Privacy Impact Assessment RGPD)

Coupler ii et iii
+ v

(attention : continuité de l'activité)

iv : Chaine de responsabilité ? indépendance ?
Modèle DPO ?
Officer + pour chaque modèle un contact
Question : séparer entre :
    - Métier (application)
    - Data Scientist (biais)
    - sponsors? Commanditaire ?

Responsabilité : possibilité de s'inspirer de la propriété intellectuelle ? analogie à voir.
Qui a fait quoi ?

License de distribution de modèle ML.

A EXPLORER

**T6 : **

A découper - environnemental vs. societal (quanti vs. quali)
Cf Responsabilité sociétale et environnementale des entreprises (loi PACTE)

A catégoriser :

    premier point : règle d'utilisation  - qui :
        - proximité des données d'entrainement
    deuxième point : interpretabilité - logique d'échelle

échelle d'Octo :
 - compréhensible ou pas ? (pour Octo, outil différents explicabilité vs. interprétabilité)
 - manière dont cela est présenté ?

Interprétabilité - complexe mais importante car demande sociétale.

 Check issues: \url{https://github.com/SubstraFoundation/referentiel-ds-responsable-confiance/issues/25}
###

###Ouverture :

A qui s'adresse le référentiel ?

(IMPACT AI \url{http://www.impact-ai.fr/ia-responsable/)}



###** Merci à tou.te.s ! \o/ **
