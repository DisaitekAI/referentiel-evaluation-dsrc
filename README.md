# Comment définir la data science responsable et de confiance ?

## Sommaire

- [Contexte, motivations et ambition](#contexte-motivations-et-ambition)
    - [Pourquoi _responsable_ et _de confiance_ ?](#pourquoi-responsable-et-de-confiance)
    - [Inspirations](#inspirations)
    - [Un référentiel de bonnes pratiques](#un-referentiel-de-bonnes-pratiques)
- [Approche participative](#approche-participative)
    - [Cycle d'ateliers de co-construction](#cycle-dateliers-de-co-construction)
    - [Disponibilité en ligne des travaux et participation asynchrone](#disponibilite-en-ligne-des-travaux-et-participation-asynchrone)
    - [Nature évolutive](#nature-évolutive)
- [Périmètres cible et hors-cible](#périmètres-cible-et-hors-cible)
- [Travaux dans ce domaine](#travaux-dans-ce-domaine)
- [Propositions de thèmes, d'un canevas du référentiel](#propositions-de-thèmes-dun-canevas-du-référentiel)

## Contexte, motivations et ambition

Un nouvel espace émerge au croisement entre expansion de l'IA dans les organisations et les systèmes automatiques, et inquiétudes du public sur les données privées, la transparence et la robustesse des algorithmes.

Ce sont deux tendances puissantes qui commencent déjà à se percuter (voir par exemple [le cas Apple Card](https://twitter.com/dhh/status/1192540900393705474)). Comment les réconcilier, les conjuguer ensemble ? Des solutions techniques et organisationnelles nouvelles sont indispensables pour cela, pour accorder un cadre de confiance qui manque aujourd’hui, pour rendre possible des collaborations nouvelles, prometteuses et sûres entre les entreprises, les institutions publiques et les citoyens.

De nombreux acteurs s'emparent du sujet et travaillent par exemple déjà à des cadres pour un usage éthique et à impact positif des technologies d'IA, à des outils pour apporter de la traçabilité aux travaux de data science, à des formations pour éviter la reproduction de biais discriminatoires, à des briques techniques pour permettre la mutualisation et renforcer la confidentialité des données, etc.

En s'appuyant sur les travaux, cadres et corpus existants, nous vous proposons de travailler de manière ouverte et collaborative à la définition de ce que serait la **data science responsable et de confiance**. L'objectif ? Établir ensemble un référentiel open source de bonnes pratiques permettant aux organisations intéressées d'évaluer leur niveau de maturité (sur le modèle de l'annexe A de la norme ISO 27001 ou du label B-Corp par exemple).

Ce référentiel devra être actionnable, opérationnel, pour que cela puisse être utile le plus rapidement possible, susciter des réflexions, des échanges, des souhaits d'amélioration. Cela pourra faciliter l'émergence d'offres d'évaluation et de formation dans ce domaine.

### Pourquoi _responsable_ et _de confiance_ ?

_Responsable_ : Qui se préoccupe des conséquences sur ses parties prenantes, cherche à avoir un impact positif, essaie d'éviter d'être _irresponsable_ c'est-à-dire ne pas maîtriser des conséquences préjudiciables pour ses parties prenantes.

_De confiance_ : Dans lequel on peut avoir un niveau de confiance raisonnable car les règles de l'art prévenant une large panoplie de risques typiques sont appliquées.

Les deux notions se recouvrent en partie. Il est cependant difficile de trouver un terme unique satisfaisant. La combinaison des deux apporte une richesse utile.

### Inspirations

- [Annexe A ISO 27001](https://fr.wikipedia.org/wiki/ISO/CEI_27001) : 114 mesures de sécurité classées en 14 catégories, dans le domaine des systèmes d'information
- [ITIL](https://fr.wikipedia.org/wiki/Information_Technology_Infrastructure_Library) : référentiel méthodologique sur l'organisation, l'efficacité, la réduction des risques, l'amélioration de la qualité des systèmes d'information
- B-Corp '[B Impact Assessment](https://bimpactassessment.net/)' : questionnaire gratuit et confidentiel d'évaluation de l'impact social et environnemental d'une organisation
- [Don en confiance](www.donenconfiance.org) : charte de déontologie et label dans le domaine du financement des associations et de l'appel public à la générosité

### Un référentiel de bonnes pratiques

- Référentiel de _bonnes pratiques_. Une bonne pratique est une pratique cible, une mesure qui peut ou non être mise en oeuvre. Par exemple voici une mesure dans le domaine des systèmes d'information et de l'évaluation ISO 27001 :
> _Des procédures de gestion des supports amovibles doivent être mises en œuvre conformément au plan de classification adopté par l’organisation._

- Chaque organisation met en oeuvre les mesures cibles à sa façon avec un certain _niveau de maturité_, qui peut évoluer dans le temps au fur et à mesure des progrès de l'organisation :

| Niveau d'implémentation | Note de maturité | Point de vue processus |
|---|:---:|---|
| Mesure non implémentée | 0 | Pratique inexistante ou incomplète |
| Mesure en cours d'implémentation | 1 | Pratique informelle |
| Mesure implémentée nécessitant amélioration | 2 | Pratique répétable et suivie |
| Mesure implémentée | 3 | Processus défini |
| Mesure implémentée et contrôlée | 4 | Processus contrôlé |
| Mesure implémentée, contrôlée et optimisée | 5 | Processus en amélioration continue |

## Approche participative

### Cycle d'ateliers de co-construction

Nous proposons de travailler de manière ouverte et collaborative et organisons un cycle d'ateliers de co-construction :

- Atelier #1 : mercredi 18 décembre 2019 à Paris
- Atelier #2 : jeudi 6 février 2020 à Paris
- Atelier #3 : jeudi 2 avril 2020 à Paris
- Atelier #4 : mardi 23 juin 2020
- Atelier #5 : mardi 8 septembre 2020
- Atelier #6 : mardi 10 novembre 2020
- Atelier #7 : mardi 15 décembre 2020

Curieux ? Enthousiaste ? Sceptique ? Essayons ensemble, avec toutes les bonnes énergies de celles et ceux qui sont intéressés par le sujet et la démarche, avec l’esprit ouvert à la possibilité que cette démarche puisse muter, rencontrer d’autres initiatives, peut-être ne pas aboutir… avec la certitude en revanche de débattre et d’apprendre sur des sujets passionnants.

### Disponibilité en ligne des travaux et participation asynchrone

Ce projet en ligne et le dépôt de fichiers associés, hébergés par Substra Foundation sur Github, assurent la disponibilité en ligne de ces travaux et du référentiel de la data science responsable et de confiance. Au-delà des ateliers participatifs bimestriels, il ainsi possible de participer de manière asynchrone.

### Nature évolutive

Par nature cette démarche est en constante évolution. Il nous semble plus utile et plus transparent de mettre à disposition le référentiel dans son état du moment, plutôt que d'attendre le franchissement de jalons majeurs. Ainsi, chacun est en mesure d'en prendre connaissance et de participer par des questions ou des suggestions d'amélioration.
Une logique de versions ou de jalons sera proposée afin de fournir un repère temporel aux organisations utilisatrices.

## Périmètres cible et hors-cible

- Un référentiel de pratiques qui s'adresse à qui ?
    - Cible : l'activité data science d'une organisation
    - Hors-cible : un projet spécifique, un produit donné, un modèle en particulier
    - Pourquoi ?
        - Les projets et produits peuvent prendre des formes extrêmement variées et il est donc très difficile d'être pertinent avec un référentiel générique
        - L'effort pour s'évaluer selon un référentiel peut être trop élevé s'il doit être fait projet par projet
        - Les mesures ou pratiques relatives aux collaborateurs (e.g. les formations) correspondent plus naturellement aux pratiques d'une organisation qu'à celle d'un projet donné
- Que désigne-t-on par _IA_ et _data science_ ?
    - Cible : l'utilisation de techniques algorithmiques sur des données, ainsi que les modèles prédictifs et les systèmes automatiques en résultant
    - Hors cible : ...

## Travaux dans ce domaine

- [EU Draft Ethics guidelines for trustworthy AI](https://ec.europa.eu/digital-single-market/en/news/draft-ethics-guidelines-trustworthy-ai) and [pilot assessment survey](https://ec.europa.eu/futurium/en/ethics-guidelines-trustworthy-ai/register-piloting-process-0)

    > 7 Key requirements:
    > - Human agency and oversight
    > - Technical robustness and safety
    > - Privacy and data governance
    > - Transparency
    > - Diversity, non-discrimination and fairness
    > - Societal and environmental well-being
    > - Accountability

- [OECD AI Principles](https://www.oecd.org/going-digital/ai/principles/) focused on 'Responsible stewardship of trustworthy AI'

    > The Recommendation identifies five complementary values-based principles for the responsible stewardship of trustworthy AI:
    > - AI should benefit people and the planet by driving inclusive growth, sustainable development and well-being.
    > - AI systems should be designed in a way that respects the rule of law, human rights, democratic values and diversity, and they should include appropriate safeguards – for example, enabling human intervention where necessary – to ensure a fair and just society.
    > - There should be transparency and responsible disclosure around AI systems to ensure that people understand AI-based outcomes and can challenge them.
    > - AI systems must function in a robust, secure and safe way throughout their life cycles and potential risks should be continually assessed and managed.
    > - Organisations and individuals developing, deploying or operating AI systems should be held accountable for their proper functioning in line with the above principles.

-  The Institute for Ethical AI & Machine Learning: [Awesome AI guidelines](https://github.com/ethicalml/awesome-artificial-intelligence-guidelines) and [The Responsible ML Principles](https://ethical.institute/principles.html):

    > The Responsible Machine Learning Principles:
    > 1. **Human augmentation**: I commit to assess the impact of incorrect predictions and, when reasonable, design systems with human-in-the-loop review processes
    > 1. **Bias evaluation**: I commit to continuously develop processes that allow me to understand, document and monitor bias in development and production.
    > 1. **Explainability by justification**: I commit to develop tools and processes to continuously improve transparency and explainability of machine learning systems where reasonable.
    > 1. **Reproducible operations**: I commit to develop the infrastructure required to enable for a reasonable level of reproducibility across the operations of ML systems.
    > 1. **Displacement strategy**: I commit to identify and document relevant information so that business change processes can be developed to mitigate the impact towards workers being automated.
    > 1. **Practical accuracy**: I commit to develop processes to ensure my accuracy and cost metric functions are aligned to the domain-specific applications.
    > 1. **Trust by privacy**: I commit to build and communicate processes that protect and handle data with stakeholders that may interact with the system directly and/or indirectly.
    > 1. **Data risk awareness**: I commit to develop and improve reasonable processes and infrastructure to ensure data and model security are being taken into consideration during the development of machine learning systems.

- [PWC IA responsable](https://www.pwc.fr/fr/vos-enjeux/data-intelligence/intelligence-artificielle/intelligence-artificielle-responsable.html):

    > 6 thèmes :
    > - Renforcer la sécurité de l'IA avec validation, surveillance et vérification
    > - Créer des modèles d'IA transparents, extensibles et prouvables
    > - Créer des systèmes éthiques, compréhensibles, légaux
    > - Améliorer la gouvernance avec des modèles d'exploitation et des processus de l'IA
    > - Tester le biais dans les données, les modèles et l'utilisation d'algorithmes par l'homme

- [Future of Life's AI principles](https://futureoflife.org/ai-principles/): endorsed by AI and tech major personalities
- [Google recommended practices for AI: Fairness, Interpretability, Privacy, Security](https://ai.google/education/responsible-ai-practices)
- [Déclaration de Montréal pour l'IA responsable](https://www.declarationmontreal-iaresponsable.com/la-declaration)
- [Serment Holberton-Turing](https://www.holbertonturingoath.org/accueil)
- [Serment d'Hippocrate pour data scientist](https://dataforgood.fr/projects/4_serment-hippocrate.html)
- [Livre blanc Data Responsable](http://www.utopies.com/fr/initiatives/groupe-de-travail-data-responsable)
- [Responsible AI Licenses](https://www.licenses.ai/)
- [Méta-étude 'The global landscape of AI ethics guidelines'](https://arxiv.org/ftp/arxiv/papers/1906/1906.11668.pdf)

Quelques observations :

- Beaucoup de travaux s'intéressent à l'éthique par les usages et par la non-reproduction de discrimination
- Il y a cependant très peu de choses sur comment un modèle est élaboré
- Le plus complet est peut-être le questionnaire d'évaluation de l'UE, mais il est loin d'être actionnable, opérationnel (63 questions dont de nombreuses sont des questions très ouvertes), et son processus d'élaboration et d'évolution est relativement fermé

## Propositions de thèmes, d'un canevas du référentiel

Quels sont les risques que l'on souhaite prévenir ?

| # | Risques | Exemples réels |
|:---:|:---|:---|
| R1 | l'exposition de données personnelles ou confidentielles, directement ou via ce qu'un modèle peut potentiellement révéler | [Re-identification de datasets "anonymisés"](https://www.wired.com/2007/12/why-anonymous-data-sometimes-isnt/), [Retro-engineering des résultats d'un algorithme](https://www.abc.net.au/news/2019-03-01/abs-census-vulnerability/10857236). |
| R2 | des "prises de décisions" par des systèmes automatiques qui seraient préjudiciables à des personnes ou des organisations du fait de leur injustice | [le cas Apple Card](https://twitter.com/dhh/status/1192540900393705474), [Algorithme RH d'Amazon](https://www.lefigaro.fr/social/2018/10/11/20011-20181011ARTFIG00096-le-logiciel-de-recrutement-d-amazon-n-aimait-pas-les-femmes.php) |
| R3 | l'utilisation de modèles prédictifs dans des contextes où leur performance réelle est insuffisante par rapport au déclaré ou à l'attendu | [L'exemple de Google Flu en médecine](https://science.sciencemag.org/content/343/6176/1203)|
| R4 | ne plus savoir comment un modèle prédictif a été élaboré (sur quelles données, avec quel algorithme d'apprentissage, à partir de quel modèle de départ) | [biais induits par les corpus de texte utilisés pour entraîner un modèle de word embedding](https://arxiv.org/abs/1607.06520) |
| R5 | ne plus savoir comment la performance d'un modèle prédictif a été évaluée (sur quelles données, selon quelle métrique de performance) | [Biais et performances limitées du modèle COMPAS de prédiction de la récidive](https://advances.sciencemag.org/content/4/1/eaao5580) |
| R6 | dans le cas d'un incident avec ou dû à un modèle prédictif : pour le sujet concerné, ne pas savoir vers qui se tourner | [cas vécu par Steve Wozniak avec l'Apple Card](https://twitter.com/stevewoz/status/1193330241478901760) |
| R7 | dans le cas d'un incident avec ou dû à un modèle prédictif : pour l'acteur qui met en oeuvre le modèle, ne savoir ni interpréter ni expliquer la prédiction en cause | [Christchurch: les algorithmes de censure sont moins efficaces pour détecter des tueries en Nouvelle Zélande que des vidéos de propagande de l'EI. Que détectent-ils fondamentalement?](https://techcrunch.com/2019/03/21/facebooks-ai-couldnt-spot-mass-murder/) |
| R8 | ne pas connaître le coût énergétique ou environnemental de l'élaboration et de l'utilisation d'un modèle, ou qu'il soit disproportionné par rapport à l'usage cible du modèle | [AlphaGo en kW vs. 20W pour un humain](https://deepmind.com/blog/article/alphago-zero-starting-scratch) |

Propositions de thèmes pour structurer les bonnes pratiques et mesures de prévention des risques :
-

| # | Thèmes | Descriptions |
|:---:|:---|:---|
| T1 | **Protéger les données personnelles ou confidentielles** | L'utilisation de données personnelles ou confidentielles fait porter le risque d'exposition de celles-ci, ce qui peut avoir des conséquences très préjudiciables pour les producteurs, gestionnaires, ou sujets de ces données. Elles doivent donc être protégées, les risques d'exposition doivent être minimisés. |
| T2 | **Prévenir les biais malencontreux** | L'utilisation de modèles prédictifs élaborés à partir de données historiques peut se révéler néfaste lorsque les données historiques décrivent des phénomènes non souhaitables. Il apparaît indispensable de s'interroger sur ce risque et d'étudier la nature des données utilisées et ce qu'elles représentent. |
| T3 | **Evaluer la performance de manière rigoureuse** | Le score de performance d'un modèle prédictif est déterminant pour son adoption dans des produits, systèmes ou processus. L'évaluation de la performance se doit donc d'être rigoureuse. |
| T4 | **Etablir et maintenir une généalogie des modèles** | Un modèle prédictif est un objet informatique complexe qui peut évoluer au fil des apprentissages. Tracer les étapes de son élaboration et de son évolution permet d'en constituer une forme de **généalogie**, pré-requis pour **reproduire ou auditer** un modèle. |
| T5 | **Garantir la chaîne de responsabilité des modèles** | Un modèle prédictif peut-être utilisé comme un système automatique, dont les règles de fonctionnement ne sont pas écrites _in extenso_ et ne se prêtent pas ou mal à être explicitées, débattues, ajustées. Des efforts sont nécessaires sur **l'interprétation et l'explication** des choix réalisés à l'aide de ces systèmes. Il apparaît également indispensable de garantir une chaîne de responsabilité claire, de personnes physiques ou morales, pour chaque modèle. |
| T6 | **Minimiser l'empreinte énergétique de l'activité data science** | Les ressources informatiques peuvent être gourmandes en énergie et avoir une empreinte environnementale significative si elles ne sont pas optimisées. Prendre conscience de ce phénomène est indispensable, chercher à mesurer, voire optimiser l'utilisation des ressources est nécessaire. |
